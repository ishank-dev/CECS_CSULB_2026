{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f19c13c-6d43-4a43-9968-73ee5c46d137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "import sys\n",
    "import ray\n",
    "\n",
    "# Initialize Ray\n",
    "ray.init(ignore_reinit_error=True, local_mode=True)\n",
    "\n",
    "# Defines the neural network architecture\n",
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTNet, self).__init__()\n",
    "        # Input layer (28x28 pixels flattened to 784)\n",
    "        self.fc1 = nn.Linear(28 * 28, 256)\n",
    "        # Hidden layers\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        # Output layer (10 classes)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        # Dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input tensor\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        # Pass through layers with ReLU activation and dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        # Output layer (logits)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Training function for Ray Tune\n",
    "def train_model(config):\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torchvision import datasets, transforms\n",
    "    from torch.utils.data import DataLoader, random_split\n",
    "    from ray.air import session  # Import session here\n",
    "    import os\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Defined transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Loaded the MNIST training dataset\n",
    "    train_dataset = datasets.MNIST(\n",
    "        root='./data',\n",
    "        train=True,\n",
    "        download=True,  # Ensure dataset is available\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    # Splited into sub-training and validation sets\n",
    "    sub_train_size = 50000\n",
    "    valid_size = 10000\n",
    "\n",
    "    # Ensured total size matches the dataset\n",
    "    assert sub_train_size + valid_size == len(train_dataset), \"Sizes do not match total dataset size.\"\n",
    "\n",
    "    # Splited the dataset\n",
    "    sub_train_dataset, valid_dataset = random_split(\n",
    "        train_dataset,\n",
    "        [sub_train_size, valid_size],\n",
    "        generator=torch.Generator().manual_seed(42)  # For reproducibility\n",
    "    )\n",
    "\n",
    "    # Created data loaders\n",
    "    batch_size = 64\n",
    "    sub_train_loader = DataLoader(sub_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialized the model\n",
    "    model = MNISTNet()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Defined the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Chose optimizer based on config\n",
    "    optimizer_name = config[\"optimizer\"]\n",
    "    if optimizer_name == \"SGD\":\n",
    "        optimizer = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=config[\"lr\"],\n",
    "            momentum=config[\"momentum\"],\n",
    "            weight_decay=config[\"weight_decay\"]\n",
    "        )\n",
    "    elif optimizer_name == \"Adagrad\":\n",
    "        optimizer = torch.optim.Adagrad(\n",
    "            model.parameters(),\n",
    "            lr=config[\"lr\"],\n",
    "            initial_accumulator_value=config[\"initial_accumulator_value\"],\n",
    "            eps=config[\"eps\"]\n",
    "        )\n",
    "    elif optimizer_name == \"RMSprop\":\n",
    "        optimizer = torch.optim.RMSprop(\n",
    "            model.parameters(),\n",
    "            lr=config[\"lr\"],\n",
    "            alpha=config[\"alpha\"],\n",
    "            momentum=config[\"momentum\"]\n",
    "        )\n",
    "    elif optimizer_name == \"Adam\":\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=config[\"lr\"],\n",
    "            betas=(config[\"beta1\"], config[\"beta2\"])\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
    "\n",
    "    # Lists to store losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for images, labels in sub_train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valid_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate average losses and accuracy\n",
    "        avg_train_loss = train_loss / len(sub_train_loader.dataset)\n",
    "        avg_val_loss = val_loss / len(valid_loader.dataset)\n",
    "        val_accuracy = correct / total\n",
    "\n",
    "        # Append losses for plotting\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Save losses after each epoch\n",
    "        trial_dir = os.getcwd()\n",
    "        torch.save({\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses\n",
    "        }, os.path.join(trial_dir, \"losses.pt\"))\n",
    "\n",
    "        # Send metrics to Ray Tune\n",
    "        session.report(\n",
    "            {\n",
    "                \"loss\": avg_val_loss,\n",
    "                \"accuracy\": val_accuracy,\n",
    "                \"training_iteration\": epoch + 1  # Report training iteration\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Defined hyperparameter search spaces for each optimizer\n",
    "# SGD\n",
    "config_sgd = {\n",
    "    \"optimizer\": \"SGD\",\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),            # γ (gamma)\n",
    "    \"momentum\": tune.uniform(0.0, 0.9),           # μ (mu)\n",
    "    \"weight_decay\": tune.loguniform(1e-5, 1e-2),  # τ (tau)\n",
    "}\n",
    "\n",
    "# AdaGrad\n",
    "config_adagrad = {\n",
    "    \"optimizer\": \"Adagrad\",\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),                    # γ (gamma)\n",
    "    \"initial_accumulator_value\": tune.uniform(0.0, 0.1),  # τ (tau)\n",
    "    \"eps\": tune.loguniform(1e-10, 1e-8),                  # η (eta)\n",
    "}\n",
    "\n",
    "# RMSProp\n",
    "config_rmsprop = {\n",
    "    \"optimizer\": \"RMSprop\",\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),   # γ (gamma)\n",
    "    \"alpha\": tune.uniform(0.9, 0.99),    # α (alpha)\n",
    "    \"momentum\": tune.uniform(0.0, 0.9),  # μ (mu)\n",
    "}\n",
    "\n",
    "# Adam\n",
    "config_adam = {\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),   # γ (gamma)\n",
    "    \"beta1\": tune.uniform(0.8, 0.99),    # β₁ (beta1)\n",
    "    \"beta2\": tune.uniform(0.9, 0.999),   # β₂ (beta2)\n",
    "}\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"accuracy\",\n",
    "    mode=\"max\",\n",
    "    max_t=5,           # Should match num_epochs\n",
    "    grace_period=5,    # Set to number of epochs to prevent early stopping\n",
    "    reduction_factor=2,\n",
    "    time_attr=\"training_iteration\"\n",
    ")\n",
    "\n",
    "# Used CLIReporter and set print_intermediate_tables to False for better notebook compatibility\n",
    "reporter = CLIReporter(\n",
    "    parameter_columns=[\"optimizer\", \"lr\", \"momentum\", \"weight_decay\", \"alpha\", \"beta1\", \"beta2\", \"initial_accumulator_value\", \"eps\"],\n",
    "    metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"],\n",
    "    print_intermediate_tables=False\n",
    ")\n",
    "\n",
    "# Function to run hyperparameter tuning\n",
    "def run_tuning(config, num_samples=10, optimizer_name=\"Optimizer\"):\n",
    "    result = tune.run(\n",
    "        train_model,\n",
    "        resources_per_trial={\"cpu\": 1},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter,\n",
    "        name=f\"{optimizer_name}_tuning\"\n",
    "    )\n",
    "\n",
    "    # Get the best trial\n",
    "    best_trial = result.get_best_trial(\"accuracy\", \"max\", \"last\")\n",
    "    print(f\"\\nBest trial for {optimizer_name}:\")\n",
    "    print(\"  Hyperparameters: {}\".format(best_trial.config))\n",
    "    print(\"  Validation Accuracy: {:.4f}\".format(best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "    # Return best\n",
    "    return best_trial.config, best_trial.local_path, best_trial.last_result[\"accuracy\"]\n",
    "\n",
    "# Function to plot training and validation loss curves\n",
    "def plot_losses(trial_dir, optimizer_name):\n",
    "    import torch\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os\n",
    "\n",
    "    # Loaded the saved losses\n",
    "    losses_path = os.path.join(trial_dir, \"losses.pt\")\n",
    "    if not os.path.exists(losses_path):\n",
    "        print(f\"No losses.pt found in {trial_dir}\")\n",
    "        return\n",
    "    losses = torch.load(losses_path)\n",
    "    train_losses = losses['train_losses']\n",
    "    val_losses = losses['val_losses']\n",
    "\n",
    "    # Ploted the losses\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
    "    plt.title(f'{optimizer_name} Loss Curves')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# test accuracy function\n",
    "def evaluate_test_accuracy(config):\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torchvision import datasets, transforms\n",
    "    from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Defined transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Loaded the MNIST training dataset\n",
    "    train_dataset = datasets.MNIST(\n",
    "        root='./data',\n",
    "        train=True,\n",
    "        download=True,  # Ensure dataset is available\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    # Splited into sub-training and validation sets\n",
    "    sub_train_size = 50000\n",
    "    valid_size = 10000\n",
    "\n",
    "    # Ensured total size matches the dataset\n",
    "    assert sub_train_size + valid_size == len(train_dataset), \"Sizes do not match total dataset size.\"\n",
    "\n",
    "    sub_train_dataset, _ = random_split(\n",
    "        train_dataset,\n",
    "        [sub_train_size, valid_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    batch_size = 64\n",
    "    sub_train_loader = DataLoader(sub_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = datasets.MNIST(\n",
    "        root='./data',\n",
    "        train=False,\n",
    "        download=True,  # Ensure dataset is available\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = MNISTNet()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer_name = config[\"optimizer\"]\n",
    "    if optimizer_name == \"SGD\":\n",
    "        optimizer = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=config[\"lr\"],\n",
    "            momentum=config[\"momentum\"],\n",
    "            weight_decay=config[\"weight_decay\"]\n",
    "        )\n",
    "    elif optimizer_name == \"Adagrad\":\n",
    "        optimizer = torch.optim.Adagrad(\n",
    "            model.parameters(),\n",
    "            lr=config[\"lr\"],\n",
    "            initial_accumulator_value=config[\"initial_accumulator_value\"],\n",
    "            eps=config[\"eps\"]\n",
    "        )\n",
    "    elif optimizer_name == \"RMSprop\":\n",
    "        optimizer = torch.optim.RMSprop(\n",
    "            model.parameters(),\n",
    "            lr=config[\"lr\"],\n",
    "            alpha=config[\"alpha\"],\n",
    "            momentum=config[\"momentum\"]\n",
    "        )\n",
    "    elif optimizer_name == \"Adam\":\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=config[\"lr\"],\n",
    "            betas=(config[\"beta1\"], config[\"beta2\"])\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
    "\n",
    "    num_epochs = 5  # Used the same number of epochs as during tuning\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for images, labels in sub_train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    return test_accuracy  # Return the test accuracy\n",
    "\n",
    "# Collect best results\n",
    "results = []\n",
    "\n",
    "# SGD\n",
    "best_sgd_config, sgd_trial_dir, val_accuracy_sgd = run_tuning(config_sgd, num_samples=5, optimizer_name=\"SGD\")\n",
    "plot_losses(sgd_trial_dir, \"SGD\")\n",
    "print(\"SGD Optimizer Test Accuracy:\", flush=True)\n",
    "test_accuracy_sgd = evaluate_test_accuracy(best_sgd_config)\n",
    "results.append({\n",
    "    'Optimizer': 'SGD',\n",
    "    'Validation Accuracy': val_accuracy_sgd,\n",
    "    'Test Accuracy': test_accuracy_sgd,\n",
    "    'Best Hyperparameters': best_sgd_config\n",
    "})\n",
    "\n",
    "# AdaGrad\n",
    "best_adagrad_config, adagrad_trial_dir, val_accuracy_adagrad = run_tuning(config_adagrad, num_samples=5, optimizer_name=\"AdaGrad\")\n",
    "plot_losses(adagrad_trial_dir, \"AdaGrad\")\n",
    "print(\"AdaGrad Optimizer Test Accuracy:\", flush=True)\n",
    "test_accuracy_adagrad = evaluate_test_accuracy(best_adagrad_config)\n",
    "results.append({\n",
    "    'Optimizer': 'AdaGrad',\n",
    "    'Validation Accuracy': val_accuracy_adagrad,\n",
    "    'Test Accuracy': test_accuracy_adagrad,\n",
    "    'Best Hyperparameters': best_adagrad_config\n",
    "})\n",
    "\n",
    "# RMSProp\n",
    "best_rmsprop_config, rmsprop_trial_dir, val_accuracy_rmsprop = run_tuning(config_rmsprop, num_samples=5, optimizer_name=\"RMSProp\")\n",
    "plot_losses(rmsprop_trial_dir, \"RMSProp\")\n",
    "print(\"RMSProp Optimizer Test Accuracy:\", flush=True)\n",
    "test_accuracy_rmsprop = evaluate_test_accuracy(best_rmsprop_config)\n",
    "results.append({\n",
    "    'Optimizer': 'RMSProp',\n",
    "    'Validation Accuracy': val_accuracy_rmsprop,\n",
    "    'Test Accuracy': test_accuracy_rmsprop,\n",
    "    'Best Hyperparameters': best_rmsprop_config\n",
    "})\n",
    "\n",
    "# Adam\n",
    "best_adam_config, adam_trial_dir, val_accuracy_adam = run_tuning(config_adam, num_samples=5, optimizer_name=\"Adam\")\n",
    "plot_losses(adam_trial_dir, \"Adam\")\n",
    "print(\"Adam Optimizer Test Accuracy:\", flush=True)\n",
    "test_accuracy_adam = evaluate_test_accuracy(best_adam_config)\n",
    "results.append({\n",
    "    'Optimizer': 'Adam',\n",
    "    'Validation Accuracy': val_accuracy_adam,\n",
    "    'Test Accuracy': test_accuracy_adam,\n",
    "    'Best Hyperparameters': best_adam_config\n",
    "})\n",
    "\n",
    "\n",
    "print(\"\\nSummary of Results:\", flush=True)\n",
    "headers = [\"Optimizer\", \"Validation Accuracy\", \"Test Accuracy\", \"Best Hyperparameters\"]\n",
    "table = []\n",
    "for result in results:\n",
    "    table.append([\n",
    "        result['Optimizer'],\n",
    "        \"{:.4f}\".format(result['Validation Accuracy']),\n",
    "        \"{:.4f}\".format(result['Test Accuracy']),\n",
    "        result['Best Hyperparameters']\n",
    "    ])\n",
    "print(tabulate(table, headers=headers, tablefmt=\"grid\"), flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aa72e7-760d-4a30-97c3-a0cabb3d29e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary of Results:\n",
    "+-------------+-----------------------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------+\n",
    "| Optimizer   |   Validation Accuracy |   Test Accuracy | Best Hyperparameters                                                                                                                |\n",
    "+=============+=======================+=================+=====================================================================================================================================+\n",
    "| SGD         |                0.9699 |          0.9755 | {'optimizer': 'SGD', 'lr': 0.048283097422401555, 'momentum': 0.8824284728982379, 'weight_decay': 0.00018708187816713113}            |\n",
    "+-------------+-----------------------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------+\n",
    "| AdaGrad     |                0.9657 |          0.9692 | {'optimizer': 'Adagrad', 'lr': 0.02272027337854904, 'initial_accumulator_value': 0.05574957840282573, 'eps': 5.440570610179236e-09} |\n",
    "+-------------+-----------------------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------+\n",
    "| RMSProp     |                0.9744 |          0.9757 | {'optimizer': 'RMSprop', 'lr': 0.0005303344832494153, 'alpha': 0.9550100132559358, 'momentum': 0.5286150762024874}                  |\n",
    "+-------------+-----------------------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------+\n",
    "| Adam        |                0.9735 |          0.9758 | {'optimizer': 'Adam', 'lr': 0.0007773168730393929, 'beta1': 0.8167293618180923, 'beta2': 0.9470870377106496}                        |\n",
    "+-------------+-----------------------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4aad8f-ae52-45ec-a425-27343d273894",
   "metadata": {},
   "source": [
    "# Explaination\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "For each optimizer, I searched over a range of hyperparameters to find the best settings that maximize validation accuracy.\n",
    "This process involved training multiple models with different hyperparameter combinations and selecting the one with the best performance.\n",
    "\n",
    "# Training and Evaluation\n",
    "- The models were trained on a large portion of the MNIST dataset (50,000 images).\n",
    "- Validation accuracy was measured on a separate set (10,000 images) to tune hyperparameters.\n",
    "- Finally, test accuracy was evaluated on the test dataset (10,000 images) that the model had not seen before.\n",
    "\n",
    "# Results Interpretation\n",
    "- **High Accuracies**: All optimizers achieved high accuracies, demonstrating that the neural network architecture is effective for the task.\n",
    "- **Optimizer Differences**: The slight differences in accuracies can be attributed to how each optimizer adjusts the model's parameters during training.\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### Adam Optimizer\n",
    "- Achieved the highest test accuracy.\n",
    "- Well-suited for problems with noisy or sparse gradients.\n",
    "- Balances learning rate adaptation with momentum.\n",
    "\n",
    "### RMSProp\n",
    "- Very close performance to Adam.\n",
    "- Good at handling non-stationary objectives (where the optimal parameters change during training).\n",
    "\n",
    "### SGD with Momentum\n",
    "- Performed admirably with a high learning rate and momentum.\n",
    "- Simple yet effective, especially when tuned properly.\n",
    "\n",
    "### AdaGrad\n",
    "- Slightly lower performance.\n",
    "- May not perform as well on non-convex problems due to aggressive learning rate decay.\n",
    "\n",
    "### Final Conclusion\n",
    "- **Model Performance**: The neural network performed exceptionally well in recognizing handwritten digits, achieving over 97% accuracy on the test set.\n",
    "- **Optimizer Choice**: While all optimizers performed well, Adam provided the best test accuracy in this experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc48097-4083-4a62-a52a-213947a03466",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
